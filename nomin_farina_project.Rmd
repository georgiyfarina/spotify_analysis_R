# Spotify top hits 1999-2019: REPORT

**Analysis of the Spotify Top Hits from 1999 to 2019.**

Georgiy Farina, Enkh-Oyu Nomin, SUPSI-DTI DS&AI 2nd year 2022/2023

## Introduction and problem setting

The dataset analysed for this project is the Spotify Top Hits collection of songs, found on [Kaggle](https://www.kaggle.com/datasets/paradisejoy/top-hits-spotify-from-20002019 "Spotify top hits 2000-2019"), between years 1999-2019. Originally it consists of 18 columns and 2000 rows, roughly 100 for each year. The features in the dataset are the following:

+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| Feature name     | Description                                                      | Range of values                                                                                                     |
+==================+==================================================================+=====================================================================================================================+
| artist           | Name of the artist / band                                        |                                                                                                                     |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| song             | Name of the song                                                 |                                                                                                                     |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| duration_ms      | Duration of a song in milliseconds                               |                                                                                                                     |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| explicit         | Indicates if a song is explicit or not                           | False / True                                                                                                        |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| year             | Release year of the song                                         |                                                                                                                     |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| popularity       | Popularity of the song                                           | From 0 to 100                                                                                                       |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| danceability     | Danceability of the song                                         | From 0 to 1                                                                                                         |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| energy           | Perceptual intensity and activity                                | From 0 to 1                                                                                                         |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| key              | The key the song is in based on the Pitch Class notation         | C = 0, C-sharp = 1, D = 2, D-sharp = 3, E = 4, F = 5, F-sharp = 6, G = 7, G-sharp = 8, A = 9, B-flat = 10, B = 11   |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| loudness         | Overall loudness of a song in decibels                           | From -60 to 0                                                                                                       |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| mode             | Modality of the song                                             | 0 = minor, 1 = major                                                                                                |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| speechiness      | Presence of spoken words in a song                               | 0 - 0.33 -\> music and other non-speech-like songs                                                                  |
|                  |                                                                  |                                                                                                                     |
|                  |                                                                  | 0.33 - 0.66 -\> may contain both music and speech (e.g: rap music)                                                  |
|                  |                                                                  |                                                                                                                     |
|                  |                                                                  | 0.66 - 1 -\> made entirely of spoken words                                                                          |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| acousticness     | Confidence measure of whether the song is acoustic               | From 0 to 1                                                                                                         |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| instrumentalness | Prediction of the song containing vocals on a scale              | From 0 to 1. A word is a vocal, but "ooh" or "aah" are not. 1 means that the song is instrumental                   |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| liveness         | Presence of an audience in the recording                         | From 0 to 1                                                                                                         |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| valence          | Measure representing the musical positiveness conveyed by a song | High values sound more positive (happy, cheerful, euphoric), Low values sound more negative (sad, depressed, angry) |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| tempo            | Overall estimated tempo of a song in BPM (beats per minute)      |                                                                                                                     |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+
| genre            | Genre/s of a song                                                | More than 1 genre can be associated to a single song                                                                |
+------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+

Based on the features present in the dataset, we wanted the following questions answered:

1.  Which artists were the most consistent throughout the years, meaning which ones had most hit songs in the charts?

2.  What is the average popularity of the top artists in the previous point?

3.  What genres are the most present and what is their average popularity?

4.  What are the top 10 songs in terms of popularity and is there a correlation between those songs and their valence?

5.  If and how did the duration of the songs change throughout the years?

6.  Is there a correlation between popularity and some features in the dataset?

The aforementioned are the questions that we wanted answered by performing exploratory data analysis, whereas the following are the two tasks we want to perform by applying regression and classification models:

1.  Predicting the popularity of a song

2.  Predicting whether a song is to be considered explicit or not

## Data cleaning and exploration

### Loading libraries and dataset

```{r}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(gridExtra)
library(tidymodels)
library(dplyr)
library(tidyr)
library(randomForest)
library(performance)
library(rsample)
library(caret)
library(discrim)
data <- read_csv('songs_normalize.csv')
data <- as_tibble(data)
summary(data)
```

### Null values and duplicated rows

Here duplicates are removed, null values are checked and variables are manipulated to be more comprehensible and manageable

Firstly we checked if there were any null values in the dataset. Then we removed the duplicate rows because there was no way of telling whether their presence meant that a song entered the top 100 in multiple years or because it was just wrongly put in the dataset.

```{r}
# Check of missing and duplicated values
sum(is.na(data))
sum(duplicated(data))

# Remove duplicated values
duplicated_rows <- duplicated(data)
data <- data[!duplicated_rows, ]
sum(is.na(data))
sum(duplicated(data))

# Remove outlying songs based on the years (there are just a couple of songs for the years 1998 and 2020)
summary(data)
```

### Removal of outlying years

We wanted to see the distribution of the songs throughout the years, once we did we observed the presence of just a small amount of songs in the years 1998 (1 song) and 2020 (3 songs), which could be dangerous to keep for further modelling, so we removed them.

```{r}
data_by_year <- data %>%
  group_by(year) %>%
  summarise(count = n())
print(data_by_year)
```

```{r}
data <- data %>%
  filter(year >= 1999 & year <= 2019)
data_by_year <- data %>%
  group_by(year) %>%
  summarise(count = n())
print(data_by_year)
```

### Rescaling and transformation on columns

We noticed that many columns with a certain supposed range (e.g: popularity, which seemingly is supposed to be from 0 to 100, in the dataset has as maximum value 89) are far from reaching the maximum supposed value. For visualization purposes we rescaled such values, in order to have a better understanding of the data.

```{r}
# Function that scales the given column to the given maximum value
rescale_col <- function(column_name, max_value, data) {
  coeff <- max(data[[column_name]])
  data[[column_name]] <- (data[[column_name]] / coeff) * max_value
  return(data)
}

# Rescale columns
data <- rescale_col("popularity", 100, data)
data <- rescale_col("acousticness", 1.0, data)
data <- rescale_col("danceability", 1.0, data)
data <- rescale_col("instrumentalness", 1.0, data)
data <- rescale_col("speechiness", 1.0, data)
```

Next step was to transform the duration from milliseconds to minutes (being more human-understandable) and to map the boolean values of the column "explicit" to 0 and 1

```{r}
data <- data %>%
  mutate(explicit = ifelse(explicit == TRUE, 1, 0)) %>%
  mutate(duration_min = duration_ms/60000)
data <- data[, -which(names(data) == "duration_ms")]
print(head(data), 5)
```

After that we also noticed that the column "genre" had in 22 of its rows the value "set()".

```{r}
unique_genres <- unique(data$genre)
print(unique_genres)
```

```{r}
num_rows <- sum(data$genre == "set()")
print(num_rows)
```

Since we had no clue about why those songs were of set() genre, we treated those songs as if they were "null", therefore we removed them in order not to influence eventual plots or models.

```{r}
data <- data[data$genre != "set()",]
num_rows <- sum(data$genre == "set()")
print(num_rows)
```

At last, having noticed a big variety of genres that a single song can have, we decided to split the "genre" column by performing one-hot-encoding and, since they become column names, we rename them by removing the spaces and special symbols.

```{r}
# Splitting the genre column into binary columns for each genre
data_long <- data %>%
  separate_rows(genre, sep = ",")

data_long$genre <- trimws(data_long$genre)

data <- data_long %>%
  mutate(value = 1) %>%
  spread(key = genre, value = value, fill = 0)


data <- data %>% rename("Dance_Electronic" = "Dance/Electronic")
data <- data %>% rename("easy_listening" = "easy listening")
data <- data %>% rename("hip_hop" = "hip hop")
data <- data %>% rename("R_and_B" = "R&B")
data <- data %>% rename("Folk_Acoustic" = "Folk/Acoustic")
data <- data %>% rename("World_Traditional" = "World/Traditional")
```

## Exploratory data analysis

### Summary Statistics

Below you can see the summary statistics after we've performed initial pre-processing and data cleaning.

```{r}
head(data)
summary(data)
```

As anticipated before, we have many boolean columns due to the split of the genre column, some variables seem to follow a normal distribution and others not. With the next chapter, we will show you the visual representation of the distribution of our features.

### Distribution of the variables

Plotting the distribution of the variables could potentially give useful insights regarding the characteristics of the hit songs. Below can be found all the plots of the numerical / boolean features

```{r , fig.width=14, fig.height=14}

#COlumns that with a binwidth of 1 would be all distributed in 1 or very few bins (e.g: columns with value between 0 and 1)
low_value_columns <- list("danceability", "energy", "speechiness", "acousticness", "instrumentalness", "valence", "duration_min", "liveness", "speechiness")
boolean_columns <- list("explicit", "mode")
genre_columns <- tail(names(data), n=14)
print(genre_columns)


plots_list <- lapply(names(data), function(column_name){
  if(is.numeric(data[[column_name]])){
    if(column_name %in% low_value_columns){
      p <- ggplot(data, aes_string(column_name)) +
        geom_histogram(binwidth = 0.05, fill = 'blue', color = 'black') +
        labs(x = column_name, y = "Count", title = paste("Histogram of", column_name))
    }else if(column_name %in% boolean_columns){
      to_plot <- as.factor(data[[column_name]])
      p <- ggplot(data, aes_string(to_plot)) +
        geom_bar() +
        scale_x_discrete(breaks=c("0", "1")) +
        labs(x=column_name, y="Count", title=paste("Distribution of", column_name), )
    }else if(column_name %in% genre_columns){
      return(NULL)
    }else{
      p <- ggplot(data, aes_string(column_name)) +
        geom_histogram(binwidth = 1, fill = 'blue', color = 'black') +
        labs(x = column_name, y = "Count", title = paste("Histogram of", column_name))
    }
    return(p)
  }
})

plots_list <- plots_list[!sapply(plots_list, is.null)]
do.call(grid.arrange, plots_list)
```

Observing the distributions of the numerical / boolean variables, we arrived to the following insights:

-   Only one quarter of the songs contain explicit content: can be due to the fact that the radios and broadcasting services may not pick the songs containing explicit lyrics and characteristics, due to the desire to spread the song among all kinds of listeners, young people included

-   There are many songs having very low values of popularity. The only reason could be that they were taken from another dataset, having a different scale. In order for the classification to work, there shouldn't be those values, which will be later removed

-   Songs having a high energy coefficient are more likely to be top hits, same goes for danceability

-   Having low speechiness, acousticness, instrumentalness and liveness can result in a bigger likelihood of getting in the top charts

### Genres distribution

Now that we observed the distribution of the numerical features, we wanted also to see which ones were the most popular genres, and we got the following results:

```{r}
genre_data <- data[, (ncol(data)-13):ncol(data)]

genre_long <- genre_data %>%
  mutate(id = row_number()) %>%
  gather(key = "Genre", value = "Count", -id)

genre_summary <- genre_long %>%
  group_by(Genre) %>%
  summarise(Count = sum(Count))

ggplot(genre_summary, aes(x = reorder(Genre, +Count), y = Count)) +
  geom_bar(stat = "identity", fill = "#800080") +
  labs(x = "Genre", y = "Count") +
  theme_bw() +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip() 
```

Out of these counts, we can notice that more than 3/4 of the songs were labeled as pop, and even though we did except a dominance or a big presence of the pop genre, we would've never guessed such dominance.

Followed by pop we have hip hop and R&B, Dance/Electronic and Rock. In the top 5 genres we were expecting to get latin music, which is a very popular kind of music especially in the warm seasons of the year, but to our surprise it's ranked just at the 7th place.

### Genres average popularity

Having observed the distribution of the genres, we wanted to find out the average popularity of those genres. The plot that we obtained is the following:

```{r}
popularity_index <- which(names(data) == "popularity")
genre_data <- data[, c(popularity_index, (ncol(data)-13):ncol(data))]

genre_long <- genre_data %>%
  mutate(id = row_number()) %>%
  gather(key = "Genre", value = "Value", -id, -popularity)

# Filter out rows where Value is 0 (i.e., the song does not belong to the genre)
genre_long <- genre_long[genre_long$Value == 1, ]

# Calculate average popularity for each genre
genre_avg_popularity <- genre_long %>%
  group_by(Genre) %>%
  summarise(Avg_Popularity = mean(popularity))

genre_avg_popularity_ordered <- merge(genre_summary, genre_avg_popularity, by.x = "Genre", by.y = "Genre")

# Plot
ggplot(genre_avg_popularity_ordered, aes(x = reorder(Genre, +Count), y = Avg_Popularity)) +
  geom_line(group = 1) +
  labs(x = "Genre", y = "Average Popularity") +
  theme_bw() +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()
```

The aforementioned top 5 of the genres is seen to have an average popularity lower than expected, between 60 and 70. There are several assumptions that could be done regarding the reason behind these results: one could be that for more common genres such pop, hip-hop or R&B there is a lower popularity value needed, since it is "easier" for those songs to enter the charts; another one could be that the popularity could be calculated as active_listeners / total_listeners (e.g: a pop is listened by 100'000 people, out of which only 1000 really listen actively to that song, whereas a metal song is listened by 1000, but of those 800 listen many times to that song). There is way of knowing for sure which one is right or not, therefore no conclusions can be made with certainty

### Top 10 artists with more songs in the dataset

Having thoroughly analysed genres and their popularities, now it's the turn of the artist to be explored. Here can be seen a plot showing the 10 artists that have released more hit songs throughout the years.

```{r}
# Count the frequency of each artist
artist_counts <- head(sort(table(data$artist), decreasing = TRUE), 10)

# Convert to data frame for plotting
artist_counts_df <- as.data.frame(artist_counts)
names(artist_counts_df) <- c("artist", "count")

# Plot
ggplot(artist_counts_df, aes(x = reorder(artist, +count), y = count)) +
  geom_bar(stat = "identity", fill = "#800080") +
  labs(x = "Artist", y = "Count") +
  theme_bw() +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()
```

Rihanna and Drake are both in a tie with 23 songs that hit the charts, followed by Eminem with 21 and by Calvin Harris with 20. After Harris we find Britney Spears and David Guetta with respectively 19 and 18 songs, with the other 4 artists to make it to the top 10 are Katy Perry, Kanye West, Chris Brown and BeyoncÃ¨, all of which have 16 hit songs.

### Average popularity of top 10 artists

Same as for the genres, we wanted to see what is the average popularity of the top artists in the dataset

```{r}
# Calculate average popularity for each artist
avg_popularity <- data %>%
  group_by(artist) %>%
  summarise(avg_popularity = mean(popularity, na.rm = TRUE))

# Merge with artist_count_df to maintain the same order
avg_popularity_ordered <- merge(artist_counts_df, avg_popularity, by.x = "artist", by.y = "artist")

# Plot
ggplot(avg_popularity_ordered, aes(x = reorder(artist, +count), y = avg_popularity)) +
  geom_line(group = 1) +
  labs(x = "Artist", y = "Average Popularity") +
  theme_bw() +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()
```

If Rihanna and Eminem meet the expectations of having an averagely higher popularity w.r.t the number of songs, there are others like Drake, Britney Spears and others who fall a bit below the expected popularity, as confirmed also from the approximated linear regression below, based on the amount of songs and on the popularity.

```{r}
# Join the two dataframes on the "artist" column
data_artists_vs_popularity <- inner_join(artist_counts_df, avg_popularity, by = "artist")

# Plot data
ggplot(data_artists_vs_popularity, aes(x = count, y = avg_popularity, label = artist)) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_point(size = 2, color = 'steelblue') +
  geom_text(size = 4)+
  labs(
    title = "Number of Songs vs Average Popularity for Top 10 Artists",
    x = "Number of Songs",
    y = "Average Popularity"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )
```

### Average duration of the songs per year

Since it's evident that time has changed and shortened averagely the attention-span of a listener, we wanted to confirm if such change in pattern would also apply for the case of the average duration of the songs per year. Below a plot showcasing the answer to the question, which confirms our expectations

```{r}
# Plot YEAR vs DURATION 

# Calculate the mean duration by year
year_data <- aggregate(duration_min ~ year, data, mean)
ggplot(year_data, aes(x = year, y = duration_min)) +
  geom_line(stat = "identity", fill = "steelblue") +
  labs(x = "Year", y = "Duration (minutes)") +
  theme_bw() +
  theme(text = element_text(size = 20))
```

As can be seen, with the years there is an evident decrease in average duration of the songs, which in the early 2000s was around 4 minutes if not more, whereas in the end of the last decade the average duration in minutes got to a little bit over 3 minutes. We can assume that this is due to the shortage of the attention span of the average listener, which nowadays craves immediate pleasure (see also social media and the duration of the videos on them) which they can also look for in the songs, and the longer the song is, the less the average audience is happy about it.

### Top 10 most popular songs

Having now analysed every top 10 possible, the last one remaining that one could ask is for the top 10 of the most popularly evaluated songs. The results are quite surprising:

```{r}
# PlOT 10 MOST POPULAR SONGS

# Select the top 10 songs based on popularity and sort in descending order


# Subset and process data
top_songs <- data %>%
  select(artist, song, popularity, valence) %>%
  mutate(artist_song = paste(artist, song, sep = " - ")) %>%
  arrange(desc(popularity)) %>%
  head(10)

# Plot data
ggplot(top_songs, aes(x = reorder(artist_song, -popularity), y = popularity)) +
  geom_line(stat = 'identity', fill = '#800080', group=1) +
  labs(
    title = "Top 10 Popular Songs",
    x = "Artist - Song",
    y = "Popularity"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8)
  )
```

Despite the age or the musical culture of each person, these songs were certainly not the ones expected as far as popularity goes. Even though many songs out of this top 10 are certainly recognizable by the majority of music listeners, there are many that even an up-to-date person might not recognize, like the two The Neighbourhood songs or the WILLOW one. This assumption doesn't want to undermine these songs, but there are many other candidates for the big top 10, like Pharrel Williams - Happy, or Someone Like You by Adele, and many others.

A possible reason that we found for such a popularity evaluation is that, after studying these songs, they are very popular especially on social media such as Instagram, Twitter and so on. Those single listens might influence the overall popularity, which may in fact include the social media interaction with those songs.

### Valence of top 10 songs

Out of the many features characterizing every song, we wanted to investigate a possible correlation of ending in the top 10 popular songs and having a certain valence, which indicates whether the song is cheerful/happy or sad/depressed. To understand it we picked the top 10 songs previously found and we plotted their value of valence

```{r}
ggplot(top_songs, aes(x = reorder(artist_song, -popularity), y = valence)) +
  geom_line(stat = 'identity', fill = '#800080', group=1) +
  labs(
    title = "Valence of top 10 Popular Songs",
    x = "Artist - Song",
    y = "Valence"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
  )
```

As can be observed, for a song is not important to be giving out happy or sad vibes, since half of the songs have a low value of valence (\< 0.5) and the other half has a high valence (\> 0.5)

### Correlation matrix

As last visualization we decided to go for the general correlation matrix, which gives us the possibility of seeing whether there exists a strong correlation between one or more features.

```{r}
# Correlation matrix

# Select only numeric columns from the dataset
numeric_data <- data[, sapply(data, is.numeric)]
numeric_data <- numeric_data[,1:(ncol(numeric_data)-14)]

cor_matrix <- cor(numeric_data)

corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", 
         tl.cex = 0.7, tl.col = "black")
```

The final results don't indicate anything regarding any correlation for the popularity column, which is the one that will be chosen as target variable for the regression model, therefore it is expectable from the model to perform poorly. As far as other correlations go (leaving aside obvious correlations such as energy-loudness), there can be seen some light correlations between the future classification target variable explicit and others like speechiness, danceability and energy.

## Modelling: Regression methods for predicting the popularity of a song

### Pre-Processing

Before proceeding to the modelling, some pre-processing is needed to ensure that the data is on the same scale and that it's ready for the various models trainings.

For this task we performed a standardization on the numerical data, removed the categorical columns "artist" and "song" and removed the data having popularity's outliers, since there are many that revolve between 0 and 1. We havent removed all the rows containing outliers because the training dataset would be reduced to approximately 450-500 rows, which is a pretty low amount.

```{r}
# Popularity outliers removal
print(paste("Values of popularity between 0 and 1:",sum(data$popularity <= 1)))
# For each column, remove rows where the column's value is an outlier
Q1 <- quantile(data[["popularity"]], 0.25, na.rm = TRUE)
Q3 <- quantile(data[["popularity"]], 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
# below in model_data filtering: | model_data[["popularity"]] > (Q3 + 1.5 * IQR))
data <- data[!(data[["popularity"]] < (Q1 - 1.5 * IQR) | data[["popularity"]] > (Q3 + 1.5 * IQR)), ]
print(paste("Min popularity after outliers removal:", min(data$popularity)))
```

TAfter outliers in popularity are removed, then the split into train and test is done with a proportion of 0.75 and 0.25 respectively for train and test, and the features artist and song are removed.

```{r}
set.seed(24)
# Removal of artist and song features
model_data <- data %>% select(-artist, -song)

data_split <- model_data %>% initial_split(prop=0.75)
train_data <- training(data_split)
test_data <- testing(data_split)
```

Once the split is performed, then standardization is applied to make sure that all the features are on the same scale, having 0 as mean and 1 as standard deviation. This process is applied to only numerical non-binary features.

```{r}
# Standardization of data
binary_cols <- sapply(train_data, function(x) all(x %in% c(0, 1)))
binary_features <- names(train_data)[binary_cols]

numeric_cols <- c("year", "danceability", "energy", "key", "loudness", "speechiness",
                  "acousticness", "instrumentalness", "liveness", "valence", "tempo", "duration_min")

reg_recipe <- recipe(popularity~., data = train_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_of(numeric_cols), -all_of(binary_features))

reg_recipe <- prep(reg_recipe, training = train_data)
train_data <- bake(reg_recipe, new_data = train_data)
test_data <- bake(reg_recipe, new_data = test_data)

summary(train_data)
```

### Linear Regression

Here the Linear Regression model is trained. The training is made by the following steps:

Next, the model itself by specifying the engine and the mode

```{r}
lin_reg_spec <- linear_reg() %>% set_engine("lm") %>%
  set_mode("regression")
```

The model specification needs then to be combined with the recipe into a single workflow function, that can be fit to the data all at once

```{r}
lin_reg_wflow <- workflow() %>% add_recipe(reg_recipe) %>%
  add_model(lin_reg_spec)

lin_reg_fit_wflow <- lin_reg_wflow %>% fit(train_data)
```

Once the model is fitted, we can extract the details of the model object from the workflow.

```{r, fig.width=10, fig.height=10}
lin_reg_fit_engine <- lin_reg_fit_wflow %>%
  extract_fit_engine()
lin_reg_fit_engine %>% summary()


lin_reg_fit_engine %>% check_model()
```

Lastly we can get the metrics of the model once it was used to predict the test_data

```{r, fig.width=7, fig.height=7}
lin_reg_predictions <- predict(lin_reg_fit_wflow, new_data = test_data) %>%
  bind_cols(test_data)

# Calculate RMSE
lin_reg_metrics <- metrics(lin_reg_predictions, truth = popularity, estimate = .pred)
print(lin_reg_metrics)
```

### Lasso Regression

Compared to the classic linear regression, Lasso takes care of distributing the weights and performing feature selection by possibly assigning a weight of 0 to the feature, if considered useless for the model.

Since there is a parameter that could be tuned, the L1 penalty, we performed a grid search that fins the best penalty value and a 10 fold cross-validation.

As before, a workflow is created that combines the formula of prediction and the model specification.

Once the tuning of the parameter is performed, the best model is returned and used to predict the test_data

```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lasso_grid <- grid_regular(
  penalty(range = c(0.001, 0.1)),
  levels = 10
)

cv <- vfold_cv(train_data, v = 10)

lasso_workflow <- workflow() %>%
  add_model(lasso_spec) %>%
  add_formula(popularity ~ .)

lasso_results <- tune_grid(
  lasso_workflow,
  resamples = cv,
  grid = ridge_grid
)

best_lasso_params <- lasso_results %>%
  select_best(metric = "rmse")

best_lasso_model <- lasso_spec %>%
  finalize_model(best_lasso_params)

best_lasso_fit <- fit(best_lasso_model, popularity ~ ., data = train_data)

best_lasso_preds <- predict(best_lasso_fit, new_data = test_data)
best_lasso_metrics <- metrics(bind_cols(test_data, best_lasso_preds), truth = popularity, estimate = .pred)

print(best_lasso_metrics)
```

### Ridge Regression

In this dataset both Lasso and Ridge regression models make sense to be used, that is because Lasso annihilates useless features and automatically performs feature selection by assigning weight 0 to the features. Ridge on the other hand is useful in case of presence of linear relations between predictor variables, which is the case here as we saw from the confusion matrix.

The implementation of the model is exactly the same as for the Lasso, but instead of the parameter mixture assigned to 1 when creating the model, for the model to apply L2 penalty we need to set mixture to 0.

```{r}
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet")

ridge_grid <- grid_regular(
  penalty(range = c(0.001, 0.1)),
  levels = 10
)

cv <- vfold_cv(train_data, v = 10)

ridge_workflow <- workflow() %>%
  add_model(ridge_spec) %>%
  add_formula(popularity ~ .)

ridge_results <- tune_grid(
  ridge_workflow,
  resamples = cv,
  grid = ridge_grid
)

best_ridge_params <- ridge_results %>%
  select_best(metric = "rmse")

best_ridge_model <- ridge_spec %>%
  finalize_model(best_ridge_params)

best_ridge_fit <- fit(best_ridge_model, popularity ~ ., data = train_data)

best_ridge_preds <- predict(best_ridge_fit, new_data = test_data)
best_ridge_metrics <- metrics(bind_cols(test_data, best_ridge_preds), truth = popularity, estimate = .pred)

print(best_ridge_metrics)
```

### Decision Tree Regression

Usually a Decision Tree is used for classification tasks, but it can also be used for regression problems, which is what we do here. That is because it can catch non-linear relationships between predictors and target, and another good aspect is the interpretability if needed.

Same procedure as for the Lasso Regression: specification of the engine, tuning of the parameters, cross-validation, evaluation of the best obtained model on the test_data

```{r}
# Specify a Decision Tree model
tree_spec <- decision_tree(mode = "regression", tree_depth = tune(), min_n = tune()) %>%
  set_engine("rpart")

# Define a grid of hyperparameters to tune
tree_grid <- grid_regular(
  tree_depth(range = c(1, 20)),
  min_n(range = c(1, 20)),
  levels = 5
)

# Define a resampling strategy
cv <- vfold_cv(train_data, v = 5)

# Combine the model, grid, and resampling strategy in a tuning workflow
tree_workflow <- workflow() %>%
  add_model(tree_spec) %>%
  add_formula(popularity ~ .)

# Perform the tuning
tree_results <- tune_grid(
  tree_workflow,
  resamples = cv,
  grid = tree_grid
)

# Find the best parameters
best_tree_params <- tree_results %>%
  select_best(metric = "rsq")

# Update the model with the best parameters
best_tree_model <- tree_spec %>%
  finalize_model(best_tree_params)

# Retrain the model with the best parameters
best_tree_fit <- fit(best_tree_model, popularity ~ ., data = train_data)

# Evaluate the model
best_tree_preds <- predict(best_tree_fit, new_data = test_data)
best_tree_metrics <- metrics(bind_cols(test_data, best_tree_preds), truth = popularity, estimate = .pred)

print(best_tree_metrics)
```

### Random Forest Regression

A Random Forest Regression model can be preferred with respect to the other models for the same reasons as the Decision Tree (since a Random Forest is essentially a collection of many trees), meaning because that it can catch non-linear and complex relationships.

The pipeline w.r.t the previous models doesn't change.

```{r}
rf_spec <- rand_forest(mode = "regression", mtry=tune(), trees=tune(), min_n=tune()) %>%
  set_engine("randomForest")

rf_grid <- grid_regular(
  mtry(range = c(1, ncol(train_data))),
  trees(range = c(100, 500)),
  min_n(range = c(1, 100)),
  levels = 5
)

cv <- vfold_cv(train_data, v = 5)

rf_workflow <- workflow() %>%
  add_model(rf_spec) %>%
  add_formula(popularity ~ .)

rf_results <- tune_grid(
  rf_workflow,
  resamples = cv,
  grid = rf_grid
)

best_rf_params <- rf_results %>%
  select_best(metric = "rsq")

best_rf_model <- rf_spec %>%
  finalize_model(best_rf_params)

best_rf_fit <- fit(best_rf_model, popularity ~ ., data = train_data)

best_rf_preds <- predict(best_rf_fit, new_data = test_data)

best_rf_metrics <- metrics(bind_cols(test_data, best_rf_preds), truth = popularity, estimate = .pred)

print(best_rf_metrics)
```

### Top regression models

In the cell below can be seen the two main metrics that we used to evaluate the regression model: RMSE (Root Mean Squared Error) and RSQ (R squared).

The first indicates the average error between predicted and actual values. It is calculated by taking the sum of squares of the differences between true and predicted values, and then takes the square root of such sum. It penalizes more the outliers since squaring larger errors gives back larger square root.

The second indicates the proportion of the variance for a dependent variable (target) that's explained by an independent variable or variables in a regression model (predictors).

```{r}
print(paste("Linear Regression RMSE and R^2:", lin_reg_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), ",", lin_reg_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)))
print(paste("Ridge Regression RMSE and R^2:", best_ridge_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), ",", best_ridge_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)))
print(paste("Lasso Regression RMSE and R^2:", best_lasso_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), ",", best_lasso_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)))
print(paste("Decision Tree RMSE and R^2:", best_tree_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), ",", best_tree_metrics %>% filter(.metric == "rsq") %>% pull(.estimate))) 
print(paste("Random Forest RMSE and R^2:", best_rf_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), ",", best_rf_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)))
```

From these metrics we can conclude that:

-   All the models perform very poorly on the popularity target. The assumption for why all the models cannot predict well the popularity is that a very large variety of songs, in terms of its features, can reach the same popularity, whether its a classical song or a pop song, whether its happy or sad, and so on.

-   Out of these models the best one, both for RMSE and R\^2, is the Ridge Regression model. It doesn't differ much from the classic Linear Regression, so if one had to be picked, we would go for the Linear Regression because of computational simplicity of fitting the model, since Ridge Regression requires L2 regularization

-   Surprisingly the Lasso Regression, even though it's supposed to perform feature selection, performs worse than the Linear Regression.

-   Despite having many correlations between the predicting features, both Decision Tree and Random Forest regression perform as poorly as the other models.

```{r}
lin_reg_residuals <- test_data$popularity - lin_reg_predictions$.pred
tree_residuals <- test_data$popularity - best_tree_preds$.pred
rf_residuals <- test_data$popularity - best_rf_preds$.pred
ridge_residuals <- test_data$popularity - best_ridge_preds$.pred
lasso_residuals <- test_data$popularity - best_lasso_preds$.pred

residuals_df <- data.frame(
  TrueValues = c(test_data$popularity, test_data$popularity, test_data$popularity, test_data$popularity, test_data$popularity),
  Residuals = c(lin_reg_residuals, tree_residuals, rf_residuals, ridge_residuals, lasso_residuals),
  Model = c(rep("Linear Regression", length(lin_reg_residuals)),
            rep("Decision Tree", length(tree_residuals)), 
            rep("Random Forest", length(rf_residuals)),
            rep("Ridge Regression", length(ridge_residuals)),
            rep("Lasso Regression", length(lasso_residuals)))
)
# For Linear Regression
ggplot(subset(residuals_df, Model == "Linear Regression"), aes(x = TrueValues, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  labs(title = "Residuals of Linear Regression",
       x = "True Values",
       y = "Residuals",
       color = "Model")

# For Decision Tree
ggplot(subset(residuals_df, Model == "Decision Tree"), aes(x = TrueValues, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  labs(title = "Residuals of Decision Tree",
       x = "True Values",
       y = "Residuals",
       color = "Model")

# For Random Forest
ggplot(subset(residuals_df, Model == "Random Forest"), aes(x = TrueValues, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  labs(title = "Residuals of Random Forest",
       x = "True Values",
       y = "Residuals",
       color = "Model")

# For Ridge Regression
ggplot(subset(residuals_df, Model == "Ridge Regression"), aes(x = TrueValues, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  labs(title = "Residuals of Ridge Regression",
       x = "True Values",
       y = "Residuals",
       color = "Model")

ggplot(subset(residuals_df, Model == "Lasso Regression"), aes(x = TrueValues, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  theme_minimal() +
  labs(title = "Residuals of Lasso Regression",
       x = "True Values",
       y = "Residuals",
       color = "Model")
```

From the residuals plots above, one for every model, we can observe a very clear linearity and correlation between residuals and the target's distribution: having noticed a normal distribution of the popularity column in the exploratory data analysis, we can assume that the residuals are directly proportionate to the popularity's value. The less the popularity is, the less the residuals are, and with higher popularity we have an averagely higher value of the residuals.

This problem can be due to the fact that in the two tails of the popularity distribution we have much less data, which makes the predictions less accurate, whereas in the range of popularity where there is more data, the predictions are more accurate

## Modelling: Classification methods for predicting "explicit" label

### Pre-Processing

Some pre-processing is required for the classification models. First the features artist and song are removed

```{r}
# Removal of artist and song features
model_data <- data %>% select(-artist, -song)
```

Then the split of data is performed in train and test with a 75/25 proportion and the strata = explicit is specified. Explicit is the target variable to be classified, whereas the strata parameter allows to distribute with the same percentage the data between train and test: in our case we have roughly 3:1 proportion between the 0s and the 1s in the target, so with strata both train and test will contain a 3:1 collection of target classes.

```{r}
table(data$explicit)
```

```{r}
# splitting of the data
set.seed(24)
model_data$explicit <- as.factor(model_data$explicit)
split_data <- initial_split(model_data, strata = "explicit", prop = 0.75)
train_data <- training(split_data)
test_data <- testing(split_data)
```

Once the split is done, we apply standardization on all the numerical non-binary features and create the classification recipe.

```{r}
# Standardization of data
binary_cols <- sapply(train_data, function(x) all(x %in% c(0, 1)))
binary_features <- names(train_data)[binary_cols]

numeric_cols <- c("year", "danceability", "energy", "key", "loudness", "speechiness",
                  "acousticness", "instrumentalness", "liveness", "valence", "tempo", "duration_min", "popularity")

class_recipe <- recipe(explicit~., data = train_data) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_of(numeric_cols), -all_of(binary_features))

class_recipe <- prep(class_recipe, training = train_data)
train_data <- bake(class_recipe, new_data = train_data)
test_data <- bake(class_recipe, new_data = test_data)

summary(train_data)
```

### Logistic Regression

Logistic regression predicts the probability of a sample belonging to one class or another. With a probability higher than 0.5 for a certain class A and lower than that for class B, class A is predicted.

To train the model, we perform the same procedure as we did previously for the regression models: first define the model and the engine, then the workflow with model and recipe specified and at last the model gets trained using the fit function, passing the training data as parameter.

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_reg_wflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(class_recipe)

log_reg_fit_wflow <- log_reg_wflow %>%
  fit(data = train_data)

log_reg_fit_engine <- log_reg_fit_wflow %>%
  extract_fit_engine()
log_reg_fit_engine %>% summary()
```

Once the model was trained, it can be used to predict the testing data and get back various metrics such as accuracy, sensitivity, specificity and others

```{r}
explicit_test_pred_logistic <- log_reg_fit_wflow %>% augment(test_data)
yardstick::metrics(explicit_test_pred_logistic,truth = explicit, estimate=.pred_class)
```

Then a confusion matrix can be obtained. With that matrix can be seem how does the model perform on each class in the target column explicit.

```{r}
predictions <- explicit_test_pred_logistic$.pred_class
true_values <- explicit_test_pred_logistic$explicit

cm <- table(Predictions = predictions, Actual = true_values)
print(cm)
```

As can be seen, the class 0 get predicted very well, with 285 rows out 310 predicted as 0, giving a 91% 0 class prediction accuracy, whereas the class 1, being the minority one, is more difficult to predict for the mode. In fact, the class 1 was predicted correctly for 56% of the rows labeled ad explicit, a little bit more than half of the times.

```{r}
TP <- cm[2, 2]
FP <- cm[2, 1]
FN <- cm[1, 2]

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

# Calculate F1 score
F1_log_reg <- 2 * ((precision * recall) / (precision + recall))
print(F1_log_reg)
```

The f1 score for these models is a better metric than accuracy since it's most useful in situations where there is an imbalanced dataset. For this model we got an f1-score of approximately 0.638, which is not good but not even so bad, if compared to how accurate were the regression models.

### Linear Discriminant Analysis

When predictors follow a normal distribution and have same variance, an LDA might be a useful tool.

Previously we saw that only a part of the features actually follow a normal distribution, but at the same time once we standardized the features, the Homoscedasticity assumption is met (same variance between the predictors). For that reason we decided to try and see how well does this model perform

Firstly the engine and the model are specified

```{r}
lda_spec <-  discrim_linear() %>%  set_engine('MASS')
```

Then the workflow is defined and consequentially fitted

```{r}
lda_wflow <- workflow() %>% add_recipe(class_recipe) %>% add_model(lda_spec)
lda_fit_wflow <- lda_wflow %>% fit(train_data)
lda_fit_engine <- lda_fit_wflow %>%
  extract_fit_engine()
lda_fit_engine
```

Once the model is trained we can evaluate it using the same metrics and methods as before for the Logistic Regression

```{r}
explicit_test_pred_lda <- lda_fit_wflow %>% augment(test_data)
yardstick::metrics(explicit_test_pred_lda,
                   truth = explicit,
                   estimate=.pred_class)
```

Compared to the Logistic Regression, the accuracy is a bit higher.

```{r}
predictions <- explicit_test_pred_lda$.pred_class
true_values <- explicit_test_pred_lda$explicit

cm <- table(Predictions = predictions, Actual = true_values)
print(cm)
```

From the confusion matrix we can conclude that this model predicts exactly the same values for the negative class 0, whereas it improved a little bit in predicting the positive class 1. Very similar results to the Logistic Regression, but a little bit better

```{r}
TP <- cm[2, 2]
FP <- cm[2, 1]
FN <- cm[1, 2]

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

# Calculate F1 score
F1_lda <- 2 * ((precision * recall) / (precision + recall))
print(F1_lda)
```

Since the amount of True Positive got a bit higher (from 69 to 73), the f1 score improved consequentially. It's still a poor value of f1, but it's already better than before.

### Random Forest Classification

If before we used a Random Forest for a regression task, now we are using it for Classification, which is a more common task where Random Forest would usually be used.

As before, the creation and the fitting of the model are done in the same way as the other 2

```{r}
rf_spec <- 
  rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("classification")

rf_wflow <- workflow() %>% 
  add_recipe(class_recipe) %>%
  add_model(rf_spec)

rf_fit_wflow <- rf_wflow %>% 
  fit(train_data)

rf_fit_engine <- rf_fit_wflow %>%
  extract_fit_engine()
rf_fit_engine
```

Accuracy estimation of the model after having it tested on the test data. Better than Logistic Regression but lower than LDA

```{r}
explicit_test_pred_rf <- rf_fit_wflow %>% augment(test_data)
yardstick::metrics(explicit_test_pred_rf,
                   truth = explicit,
                   estimate=.pred_class)
```

Confusion matrix computation. Values very similar to the other two models

```{r}
predictions <- explicit_test_pred_rf$.pred_class
true_values <- explicit_test_pred_rf$explicit

cm <- table(Predictions = predictions, Actual = true_values)
print(cm)
```

```{r}
TP <- cm[2, 2]
FP <- cm[2, 1]
FN <- cm[1, 2]

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

# Calculate F1 score
F1_rf <- 2 * ((precision * recall) / (precision + recall))
print(F1_rf)
```

```{r}
roc_log_reg <- roc_curve(explicit_test_pred_logistic,explicit, .pred_0,event_level = "first")
auc_log_reg <- roc_auc(explicit_test_pred_logistic,explicit, .pred_0,event_level = "first") %>% pull()

roc_rand_for <- roc_curve(explicit_test_pred_rf,explicit, .pred_0,event_level = "first")
auc_rand_for <- roc_auc(explicit_test_pred_rf,explicit, .pred_0,event_level = "first") %>% pull()

roc_lda <- roc_curve(explicit_test_pred_lda,explicit, .pred_0,event_level = "first")
auc_lda <- roc_auc(explicit_test_pred_lda,explicit, .pred_0,event_level = "first") %>% pull()


roc_df <- bind_rows(
  roc_log_reg %>% mutate(Model = paste("Logistic Regression (AUC:", round(auc_log_reg, 3), ")")),
  roc_rand_for %>% mutate(Model = paste("Random Forest (AUC:", round(auc_rand_for, 3), ")")),
  roc_lda %>% mutate(Model = paste("LDA (AUC:", round(auc_lda, 3), ")"))
)

# Plot ROC curves
ggplot(roc_df, aes(x = 1 - specificity, y = sensitivity, color = Model)) +
  geom_line() +
  labs(x = "1 - Specificity", y = "Sensitivity", color = "Models") +
  theme_minimal() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  coord_equal() +
  ggtitle("ROC Curves")
```

The three classification models have a good area under the curve, all of which are between 0.835 and 0.842, but the curve itself is not what would be exactly desired since it doesn't have a good compromise between TPR (True Positive Rate, y axis) and FPR (False Positive Rate, x axis), in fact the best one in this case would be to pick a point that can have roughly 80% of TPR and 25% of FPR.

## Summary of results and conclusions

### Research questions

Below you can find the initial research questions and their answers found through means of data exploration:

**Which artists were the most consistent throughout the years, meaning which ones had most hit songs in the charts?**

-   Rihanna and Drake are both in a tie with 23 songs that hit the charts, followed by Eminem with 21 and by Calvin Harris with 20. After Harris we find Britney Spears and David Guetta with respectively 19 and 18 songs, with the other 4 artists to make it to the top 10 are Katy Perry, Kanye West, Chris Brown and BeyoncÃ¨, all of which have 16 hit songs.

**What is the average popularity of the top artists in the previous point?**

-   Based on the scatter plot betweem number of hit songs and average popularity, we can conclude that If Rihanna and Eminem meet the expectations of having an averagely higher popularity equal to approximately 81 out of 100. W.r.t the number of songs, there are others like Drake and Britney Spears which have the lowest average popularity equal to 70 and 71.

**What genres are the most present and what is their average popularity?**

-   Out of the genre counts, we have 3/4 of the songs were labeled as pop, and even though we did except a dominance or a big presence of the pop genre, we would've never guessed such dominance.

    Followed by pop we have hip hop and R&B, Dance/Electronic and Rock. In the top 5 genres we were expecting to get latin music, which is a very popular kind of music especially in the warm seasons of the year and in the spanish speaking countries, but to our surprise it's ranked just at the 7th place.

**What are the top 10 songs in terms of popularity and is there a correlation between those songs and their valence?**

-   Despite the age or the musical culture of each person, these songs were certainly not the ones expected as far as popularity goes. Even though many songs out of this top 10 are certainly recognizable by the majority of music listeners, there are many that even an up-to-date person might not recognize, like the two The Neighbourhood songs or the WILLOW one. This assumption doesn't want to undermine these songs, but there are many other candidates for the big top 10, like Pharrel Williams - Happy, or Someone Like You by Adele, and many others.

    A possible reason that we found for such a popularity evaluation is that, after studying these songs, they are very popular especially on social media such as Instagram, Twitter and so on. Those single listens might influence the overall popularity, which may in fact include the social media interaction with those songs.

-   As can be observed, for a song is not important to be giving out happy or sad vibes and emotions, since half of the songs have a low value of valence (\< 0.5) and the other half has a high valence (\> 0.5). Therefore, we can conclude that for the popularity of the song, the valence is not a primary factor to be taken into consideration, but rather others

**If and how did the duration of the songs change throughout the years?**

-   As can be seen, with the years there is an evident decrease in average duration of the songs, which in the early 2000s was around 4 minutes if not more, whereas in the end of the last decade the average duration in minutes got to a little bit over 3 minutes. We can assume that this is due to the shortage of the attention span of the average listener, which nowadays craves immediate pleasure (see also social media and the duration of the videos on them) which they can also look for in the songs, and the longer the song is, the less the average audience is happy about it.

**Is there a correlation between popularity and some features in the dataset?**

-   The final results don't indicate anything regarding any correlation between the popularity target and its predictors, which is the one that was chosen as target variable for the regression models, therefore it is expectable, and indeed confirmed later, from the models to perform poorly. This point can be explained with two possible reasons:

    1.  Since the distribution of the popularity resembles a Gaussian, having the vast majority of the popularities concentrated in the 60-80 range of values, the models weren't able to learn well to predict low or high popularities.

    2.  As can be seen from the dataset, and also observed in the real world, the popularity of a song is a very abstract and subjective metric, and all kinds of songs can be popular or not even having the same or different characteristics. Therefore, it is difficult for the model to learn well how to predict a certain song's popularity.

### Models evaluation

#### Regression of popularity

We have trained five different models that are based on different techniques, but nevertheless all of the models perform pretty poorly, with a RMSE of around 10 and an accuracy, in terms of R\^2, that varies between 0.14 and 0.17. That is because of the reasons stated above in the previous point regarding the correlation of features. It cannot predict the exact or close to exact value, but it can provide a range in between the song might fall into when predicting its popularity (in average, having an RMSE of 10, if a songs popularity is really 70 it might predict it in the range between 60 and 80, with 10 of difference from both sides). So, if one wanted to guess the range in which a songs popularity might fall into, he could potentially rely on these models.

#### Classification of explicit label

Compared to the regression ones, the three classification models are more accurate in predicting an explicit label as correct. Even though the dataset is highly unbalanced, the models can still predict relatively well the labels. They are in average more accurate in classfying the not explicit songs as such rather than the explicit ones (which can be due to the unbalanced target distribution). Even the random forest classifier, which is considered to be one of the best ones for classification tasks, is not able to fight this issue, and its performance is very similar compared to the other 2 models.

#### 
